{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3448b5-32ec-49de-9ab2-bd3f757c880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, regexp_replace, trim, upper, lower, round, datediff, current_date\n",
    "import pandas as pd\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02f2b95-505e-496f-a3ee-f94bdaaaf1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/22 04:14:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardApproval_Cleaning\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92ea3d8-a3f7-4888-99c0-844f11dd58e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector in /opt/conda/miniconda3/lib/python3.10/site-packages (2.2.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60f9347d-484a-4fb4-ab40-175c7b910ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/22 04:15:32 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /10.128.0.4:35164 is closed\n",
      "25/03/22 04:15:32 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 0 from block manager BlockManagerId(1, my-cluster-w-0.us-central1-c.c.celtic-science-452211-u6.internal, 35911, None)\n",
      "java.io.IOException: Connection from /10.128.0.4:35164 closed\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277) ~[netty-handler-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n",
      "\tat java.lang.Thread.run(Thread.java:829) ~[?:?]\n",
      "/opt/conda/miniconda3/lib/python3.10/site-packages/pandas/io/sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gcs_bucket = \"dataproc-staging-us-central1-458263062208-tw36mmqt\"\n",
    "gcs_data_path = f\"gs://{gcs_bucket}/notebooks/jupyter/jupyter/Big Data Class Notebooks/Project/Project2/Data/application_record.csv\"\n",
    "\n",
    "application_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(gcs_data_path)\n",
    "\n",
    "mysql_config = {\n",
    "    'host': '34.122.208.22',\n",
    "    'user': 'root',\n",
    "    'password': 'BigData@12345',\n",
    "    'database': 'loan_data',\n",
    "    'table_name': 'credit_record'\n",
    "}\n",
    "\n",
    "mysql_conn = mysql.connector.connect(\n",
    "    host=mysql_config[\"host\"],\n",
    "    user=mysql_config[\"user\"],\n",
    "    password=mysql_config[\"password\"],\n",
    "    database=mysql_config[\"database\"]\n",
    ")\n",
    "\n",
    "\n",
    "query = f\"SELECT * FROM {mysql_config['table_name']}\"\n",
    "pandas_df = pd.read_sql(query, mysql_conn) # If you try to read via Spark JDBC, you will face issues. That is why using Pandas \n",
    "mysql_conn.close()\n",
    "\n",
    "# IN the project as well, follow the same and you can have upto 5 mb file in SQL server\n",
    "\n",
    "credit_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e1578-a75f-431f-9d3e-546c7768ec64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440ce6d1-339a-490c-8c8d-c40e294781f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/22 04:16:31 WARN TaskSetManager: Stage 4 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------+\n",
      "|     ID|MONTHS_BALANCE|STATUS|\n",
      "+-------+--------------+------+\n",
      "|5001711|            -3|     0|\n",
      "|5001711|            -2|     0|\n",
      "|5001711|            -1|     0|\n",
      "|5001711|             0|     X|\n",
      "|5001712|           -18|     0|\n",
      "+-------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "credit_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca77569b-fdc8-4b5d-8dc3-85de9a681d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|5008804|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           null|            2.0|\n",
      "|5008805|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           null|            2.0|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|             Working|Secondary / secon...|             Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|\n",
      "|5008808|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "|5008809|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "application_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc24831-3137-491a-a0b2-031eac4815ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handing Missing and Inconsistent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13ec65-52d8-44a8-97a2-b134a1216ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop completely empty rows\n",
    "application_df = application_df.dropna(how=\"all\")\n",
    "credit_df = credit_df.dropna(how=\"all\")\n",
    "\n",
    "'''\n",
    "This parameter controls when a row is dropped:\n",
    "\n",
    "how=\"all\" → drop the row only if all columns are NULL\n",
    "\n",
    "how=\"any\" → drop the row if any one column is NULL\n",
    "'''\n",
    "\n",
    "# Fill missing values\n",
    "application_df = application_df.fillna({\"OCCUPATION_TYPE\": \"Unknown\"})\n",
    "credit_df = credit_df.fillna({\"STATUS\": \"X\"})\n",
    "\n",
    "# Handling incorrect data (negative days should be converted to positive)\n",
    "\n",
    "application_df = application_df.withColumn(\n",
    "    \"DAYS_BIRTH\",\n",
    "    when(col(\"DAYS_BIRTH\") < 0, col(\"DAYS_BIRTH\") * -1)\n",
    "    .otherwise(col(\"DAYS_BIRTH\"))\n",
    ")\n",
    "\n",
    "application_df = application_df.withColumn(\"DAYS_EMPLOYED\", when(col(\"DAYS_EMPLOYED\") < 0, col(\"DAYS_EMPLOYED\") * -1).otherwise(col(\"DAYS_EMPLOYED\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23d1866-b36c-4511-ae88-6e7e426a8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim whitespace\n",
    "application_df = application_df.select([trim(col(c)).alias(c) for c in application_df.columns])\n",
    "\n",
    "# Standardize categorical values\n",
    "application_df = application_df.withColumn(\"CODE_GENDER\", upper(col(\"CODE_GENDER\")))\n",
    "application_df = application_df.withColumn(\"NAME_FAMILY_STATUS\", lower(col(\"NAME_FAMILY_STATUS\")))\n",
    "\n",
    "# Convert amount fields to float & round to 2 decimals\n",
    "application_df = application_df.withColumn(\"AMT_INCOME_TOTAL\", round(col(\"AMT_INCOME_TOTAL\"), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9bf7a51-67a6-4823-922f-0016d22f9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age in years\n",
    "application_df = application_df.withColumn(\"AGE\", round(col(\"DAYS_BIRTH\") / 365, 0))\n",
    "\n",
    "# Employment length in years\n",
    "application_df = application_df.withColumn(\"EMPLOYMENT_YEARS\", round(col(\"DAYS_EMPLOYED\") / 365, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43e86bee-c949-4f8a-978d-383486e67bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "| AGE|DAYS_BIRTH|\n",
      "+----+----------+\n",
      "|33.0|     12005|\n",
      "|33.0|     12005|\n",
      "|59.0|     21474|\n",
      "|52.0|     19110|\n",
      "|52.0|     19110|\n",
      "|52.0|     19110|\n",
      "|52.0|     19110|\n",
      "|62.0|     22464|\n",
      "|62.0|     22464|\n",
      "|62.0|     22464|\n",
      "|46.0|     16872|\n",
      "|46.0|     16872|\n",
      "|46.0|     16872|\n",
      "|49.0|     17778|\n",
      "|49.0|     17778|\n",
      "|49.0|     17778|\n",
      "|49.0|     17778|\n",
      "|49.0|     17778|\n",
      "|49.0|     17778|\n",
      "|29.0|     10669|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "application_df.select('AGE','DAYS_BIRTH').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20286fca-4a30-48aa-a10c-be0ce8b8a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS/jupyter/Big Data Class Notebooks/Project/Project2/Project Part 2/Data Cleaning & Transformation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7164cd6-76dc-44ad-8dc8-135c37752b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS/jupyter/Big Data Class Notebooks/Project/Project2/Project Part 2/cleaned_data/untitled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cc75327-8888-4472-849a-88dcc09176a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataproc-staging-us-central1-458263062208-tw36mmqt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcs_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdd8ecc2-abe5-470c-bd51-8938eea699c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/22 04:28:13 WARN TaskSetManager: Stage 12 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_gcs_path = f\"gs://{gcs_bucket}/notebooks/jupyter/jupyter/Big Data Class Notebooks/Project/Project2/Project_5_steps/cleaned_data/\"\n",
    "application_df.write.mode(\"overwrite\").parquet(cleaned_gcs_path + \"application_record_cleaned.parquet\")\n",
    "credit_df.write.mode(\"overwrite\").parquet(cleaned_gcs_path + \"credit_record_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10c788-948a-4d7b-8c77-cf567c36181f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
